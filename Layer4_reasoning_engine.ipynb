{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea3c0b-4341-4882-bcc4-79f93a545ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "causal_df = pd.read_csv(\"layer3_causal_scores.csv\")\n",
    "\n",
    "# Keep only top evidence\n",
    "evidence_df = causal_df[causal_df[\"is_top_evidence\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc068c7-8c2b-4dba-9424-e04bd6e928d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -----------------------------\n",
    "# Configure Google Gemini API\n",
    "# -----------------------------\n",
    "# Set your API key here or use environment variable\n",
    "GOOGLE_API_KEY = \"AIzaSyD8Lwt20hOHx7ZGd0zSN8VuvmrC1WysqZs\"  # Replace with your actual API key\n",
    "# Or use: import os; GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Initialize the model (using free tier model)\n",
    "MODEL_NAME = \"gemini-2.5-flash\"  # Free tier model\n",
    "\n",
    "# -----------------------------\n",
    "# Load Layer 3 evidence CSV\n",
    "# -----------------------------\n",
    "causal_df = pd.read_csv(\"layer3_causal_scores.csv\")\n",
    "evidence_df = causal_df[causal_df[\"is_top_evidence\"] == True]\n",
    "\n",
    "# -----------------------------\n",
    "# Functions to get evidence\n",
    "# -----------------------------\n",
    "def get_evidence(transcript_id):\n",
    "    subset = evidence_df[evidence_df[\"transcript_id\"] == transcript_id]\n",
    "    return subset.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "def build_prompt(transcript_id, question):\n",
    "    evidence = get_evidence(transcript_id)\n",
    "    if evidence.empty:\n",
    "        return None\n",
    "    evidence_text = \"\\n\".join(\n",
    "        f\"Turn {r.turn_id} ({r.speaker}): {r.text}\"\n",
    "        for _, r in evidence.iterrows()\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Key causal evidence:\n",
    "{evidence_text}\n",
    "\n",
    "Explain the outcome using ONLY this evidence.\n",
    "Be structured and analytical.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------\n",
    "# Explain using Google Gemini API\n",
    "# -----------------------------\n",
    "def explain(transcript_id, question):\n",
    "    prompt = build_prompt(transcript_id, question)\n",
    "    if prompt is None:\n",
    "        return \"No evidence found for this transcript.\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(MODEL_NAME)\n",
    "        \n",
    "        # Generate response\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "# explanation = explain(\"transcript_123\", \"Why did the negotiation fail?\")\n",
    "# print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8170110-c4db-41df-b5ed-43de65900164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\adityaacer7\\appdata\\roaming\\python\\python313\\site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af9112ca-43c6-498b-b132-ccd8f57581cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "causal_df = pd.read_csv(\"layer3_causal_scores.csv\")\n",
    "evidence_df = causal_df[causal_df[\"is_top_evidence\"] == True]\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "class ConversationContext:\n",
    "    \"\"\"Manages multi-turn conversation context for analytical queries.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.current_transcript_id = None\n",
    "        self.evidence_cache = None\n",
    "    \n",
    "    def add_turn(self, question, response):\n",
    "        \"\"\"Add a Q&A turn to the conversation history.\"\"\"\n",
    "        self.history.append({\n",
    "            \"question\": question,\n",
    "            \"response\": response\n",
    "        })\n",
    "    \n",
    "    def get_context_summary(self):\n",
    "        \"\"\"Generate a summary of previous turns for context.\"\"\"\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        \n",
    "        context = \"Previous conversation:\\n\"\n",
    "        for i, turn in enumerate(self.history, 1):\n",
    "            context += f\"\\nQ{i}: {turn['question']}\\n\"\n",
    "            context += f\"A{i}: {turn['response'][:300]}...\\n\"  # Truncate long responses\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        self.current_transcript_id = None\n",
    "        self.evidence_cache = None\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "def get_evidence(transcript_id):\n",
    "    \"\"\"Retrieve top evidence for a transcript.\"\"\"\n",
    "    subset = evidence_df[evidence_df[\"transcript_id\"] == transcript_id]\n",
    "    return subset.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "def build_prompt(transcript_id, question, context_summary=\"\", is_followup=False):\n",
    "    \"\"\"Build prompt with evidence and conversation context.\"\"\"\n",
    "    evidence = get_evidence(transcript_id)\n",
    "    if evidence.empty:\n",
    "        return None\n",
    "    \n",
    "    evidence_text = \"\\n\".join(\n",
    "        f\"Turn {r.turn_id} ({r.speaker}): {r.text}\"\n",
    "        for _, r in evidence.iterrows()\n",
    "    )\n",
    "    \n",
    "    if is_followup and context_summary:\n",
    "        prompt = f\"\"\"\n",
    "{context_summary}\n",
    "\n",
    "Current Follow-up Question: {question}\n",
    "\n",
    "Key causal evidence from the transcript:\n",
    "{evidence_text}\n",
    "\n",
    "Instructions:\n",
    "- This is a follow-up question based on the previous conversation above\n",
    "- Use the same evidence and maintain consistency with previous analysis\n",
    "- Build upon earlier responses where relevant\n",
    "- If the question references previous analysis (e.g., \"that point\", \"the reason you mentioned\"), explicitly connect to prior responses\n",
    "- Be structured and analytical\n",
    "- Only use the provided evidence\n",
    "\n",
    "Provide your response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Key causal evidence from the transcript:\n",
    "{evidence_text}\n",
    "\n",
    "Instructions:\n",
    "- Explain the outcome using ONLY this evidence\n",
    "- Be structured and analytical\n",
    "- Focus on causal relationships\n",
    "\n",
    "Provide your response:\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "class MultiTurnExplainer:\n",
    "    \"\"\"Handles multi-turn analytical conversations with context preservation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.context = ConversationContext()\n",
    "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
    "    \n",
    "    def explain(self, transcript_id, question, is_followup=False):\n",
    "        \"\"\"\n",
    "        Generate explanation with context awareness.\n",
    "        \n",
    "        Args:\n",
    "            transcript_id: ID of the transcript to analyze\n",
    "            question: User's question\n",
    "            is_followup: Whether this is a follow-up question\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with response and metadata\n",
    "        \"\"\"\n",
    "        # Check if switching transcripts\n",
    "        if self.context.current_transcript_id != transcript_id:\n",
    "            if self.context.current_transcript_id is not None:\n",
    "                print(f\"Warning: Switching from transcript {self.context.current_transcript_id} to {transcript_id}. Context reset.\")\n",
    "            self.context.reset()\n",
    "            self.context.current_transcript_id = transcript_id\n",
    "        \n",
    "        # Build context summary for follow-ups\n",
    "        context_summary = \"\"\n",
    "        if is_followup:\n",
    "            context_summary = self.context.get_context_summary()\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = build_prompt(transcript_id, question, context_summary, is_followup)\n",
    "        if prompt is None:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"response\": \"No evidence found for this transcript.\",\n",
    "                \"transcript_id\": transcript_id,\n",
    "                \"question\": question\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.model.generate_content(prompt)\n",
    "            response_text = response.text\n",
    "            \n",
    "            # Add to conversation history\n",
    "            self.context.add_turn(question, response_text)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response_text,\n",
    "                \"transcript_id\": transcript_id,\n",
    "                \"question\": question,\n",
    "                \"turn_number\": len(self.context.history),\n",
    "                \"is_followup\": is_followup\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"transcript_id\": transcript_id,\n",
    "                \"question\": question\n",
    "            }\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Return the full conversation history.\"\"\"\n",
    "        return self.context.history\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset the conversation context.\"\"\"\n",
    "        self.context.reset()\n",
    "        print(\"Conversation context reset.\")\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "def explain(transcript_id, question):\n",
    "    \"\"\"\n",
    "    Original single-turn explain function (backward compatible).\n",
    "    For multi-turn use, use MultiTurnExplainer class instead.\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(transcript_id, question)\n",
    "    if prompt is None:\n",
    "        return \"No evidence found for this transcript.\"\n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL_NAME)\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71219495-0864-47be-ad74-133d25b6bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605a005f-2530-4b0c-99d3-0ce8278cda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = MultiTurnExplainer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e8f65d0-1377-4998-8e7a-18390ae39606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 response:\n",
      "Based on the provided evidence, the customer requested a replacement due to an issue with their original order, which caused inconvenience and required resolution.\n",
      "\n",
      "Here's the causal breakdown:\n",
      "\n",
      "*   **Request for Replacement:** Turn 10 (\"I can get a replacement shipped out today...\") and Turn 14 (\"Your replacement order confirmation...\") confirm that a replacement is being processed, directly implying the customer either requested it or the situation necessitated one.\n",
      "*   **Evidence of an Issue/Inconvenience:**\n",
      "    *   The agent offers \"expedited delivery at no extra charge\" (Turn 10). Waiving extra charges for expedited shipping is a common service recovery measure when a customer has experienced a problem with an initial order (e.g., delay, damage, non-delivery).\n",
      "    *   The agent adds \"a note to use signature confirmation\" (Turn 10). This measure is often employed to ensure secure delivery, especially if there was an issue with the previous delivery not reaching the customer, being lost, or stolen.\n",
      "    *   The agent expresses \"Thank you for your patience and understanding\" (Turn 14). This phrase is explicitly used when a customer has endured a difficult situation or inconvenience, indicating that the original order did not proceed as expected and caused frustration for the customer.\n",
      "\n",
      "Therefore, the customer requested a replacement as a corrective action for an unspecified problem with their initial order, likely related to its delivery or receipt, which caused them inconvenience and necessitated special handling for the replacement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Why did the customer request a replacement?\"\n",
    "r1 = explainer.explain(\"6794-8660-4606-3216\", q1, is_followup=False)\n",
    "print(f\"Turn 1 response:\\n{r1['response']}\\n{'-'*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2552077-54a7-4ce4-a581-115bbbdeb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 2 response:\n",
      "Based on the provided evidence, there is no mention of any recurring issue or previous complaints. The transcript snippets focus solely on the current replacement request and its resolution, without any reference to past problems or a history of similar issues for this customer.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2 = \"Was there any recurring issue mentioned in previous complaints?\"\n",
    "r2 = explainer.explain(\"6794-8660-4606-3216\", q2, is_followup=True)\n",
    "print(f\"Turn 2 response:\\n{r2['response']}\\n{'-'*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99187b99-73d8-4f55-83f7-d127a1dc0736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 3 response:\n",
      "Based on the provided evidence, the complaint itself, which led the customer to request a replacement, inherently indicates an initial negative impact on customer satisfaction. The agent's remark, \"Thank you for your patience and understanding\" (Turn 14), further suggests that the customer experienced some inconvenience or frustration, requiring patience.\n",
      "\n",
      "However, the agent's actions are clearly aimed at mitigating this negative impact and recovering customer satisfaction through a proactive and generous resolution. The agent offered (Turn 10):\n",
      "*   A replacement shipped out the same day.\n",
      "*   Expedited delivery at no extra charge.\n",
      "*   Delivery within 2-3 business days.\n",
      "*   The addition of signature confirmation.\n",
      "\n",
      "These measures are designed to quickly and efficiently resolve the customer's issue, provide added value (expedited delivery at no cost), and offer reassurance (signature confirmation), thereby demonstrating a commitment to customer service. While the transcript does not show the customer's direct response to this resolution, these actions are intended to restore or even enhance the customer's satisfaction despite the initial problem.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q3 = \"How does this complaint impact overall customer satisfaction?\"\n",
    "r3 = explainer.explain(\"6794-8660-4606-3216\", q3, is_followup=True)\n",
    "print(f\"Turn 3 response:\\n{r3['response']}\\n{'-'*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e44d65-854a-4cde-a59c-d0ef7c2fad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query Id                                           Query  \\\n",
      "0         1         Why did the customer raise a complaint?   \n",
      "1         2  How does this complaint impact customer trust?   \n",
      "2         3         Why did the customer raise a complaint?   \n",
      "3         4  How does this complaint impact customer trust?   \n",
      "4         5         Why did the customer raise a complaint?   \n",
      "\n",
      "        Query Category                                      System Output  \\\n",
      "0  Root Cause Analysis  The customer raised a complaint because a \"sec...   \n",
      "1    Impact Assessment  The complaint likely had a **mixed impact on c...   \n",
      "2  Root Cause Analysis  The customer raised a complaint because they w...   \n",
      "3    Impact Assessment  The complaint, stemming from the customer's in...   \n",
      "4  Root Cause Analysis  Based on the provided evidence, the customer r...   \n",
      "\n",
      "                                      Remarks  \n",
      "0    Transcript 8091-3291-7899-8020 | Initial  \n",
      "1  Transcript 8091-3291-7899-8020 | Follow-up  \n",
      "2    Transcript 6867-1697-7694-3607 | Initial  \n",
      "3  Transcript 6867-1697-7694-3607 | Follow-up  \n",
      "4    Transcript 3097-8312-6316-8913 | Initial  \n",
      "\n",
      "✅ CSV saved as multi_transcript_context_queries.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transcript_ids = [\n",
    "    \"8091-3291-7899-8020\",\n",
    "    \"6867-1697-7694-3607\",\n",
    "    \"3097-8312-6316-8913\",\n",
    "    \"7172-3330-4808-3051\",\n",
    "    \"3973-3477-5942-7055\",\n",
    "    \"6679-5013-8263-6597\",\n",
    "    \"1363-9683-3428-7159\",\n",
    "    \"5928-8023-8078-6946\",\n",
    "    \"7167-8782-2387-9231\",\n",
    "    \"9270-1429-5425-3028\"\n",
    "]\n",
    "\n",
    "# Two queries per transcript → 20 total\n",
    "query_templates = [\n",
    "    (\"Why did the customer raise a complaint?\", \"Root Cause Analysis\"),\n",
    "    (\"How does this complaint impact customer trust?\", \"Impact Assessment\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "query_id = 1\n",
    "\n",
    "for tid in transcript_ids:\n",
    "    \n",
    "    # New context per transcript\n",
    "    explainer = MultiTurnExplainer()\n",
    "    \n",
    "    for i, (query, category) in enumerate(query_templates):\n",
    "        \n",
    "        is_followup = i > 0\n",
    "        \n",
    "        res = explainer.explain(\n",
    "            tid,\n",
    "            query,\n",
    "            is_followup=is_followup\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"Query Id\": query_id,\n",
    "            \"Query\": query,\n",
    "            \"Query Category\": category,\n",
    "            \"System Output\": res[\"response\"],\n",
    "            \"Remarks\": f\"Transcript {tid} | {'Follow-up' if is_followup else 'Initial'}\"\n",
    "        })\n",
    "        \n",
    "        query_id += 1\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"multi_transcript_context_queries1.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ CSV saved as multi_transcript_context_queries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205100d-5206-4694-bd43-558f988240f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
